{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":126118,"databundleVersionId":14933142,"isSourceIdPinned":false}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Beyond Visible Spectrum: AI for Agriculture 2026 — Task 2\n## Full Pipeline: SSL Pretraining → Fine-Tuning → Ensemble\n\n**Strategy:**\n- Stage 1: MAE pretraining on unlabeled Sentinel-2 (12-band) data\n- Stage 2: Fine-tune 3 models (ViT-Base, Swin-Tiny, ConvNeXt-Small)\n- Stage 3: Weighted soft-voting ensemble + TTA\n\n**Hardware:** Optimized for Kaggle T4 x2 (16GB VRAM each)\n\n**Target:** 0.90+ accuracy on crop disease classification (Aphid, Rust, RPH, Blast)","metadata":{}},{"cell_type":"code","source":"# Install dependencies\n!pip install -q timm einops rasterio scikit-learn torchmetrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:46:50.008688Z","iopub.execute_input":"2026-02-25T16:46:50.009129Z","iopub.status.idle":"2026-02-25T16:46:53.964833Z","shell.execute_reply.started":"2026-02-25T16:46:50.009096Z","shell.execute_reply":"2026-02-25T16:46:53.963813Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport numpy as np\nimport pandas as pd\nimport json\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport rasterio\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport torchvision.transforms as T\nimport timm\nfrom einops import rearrange\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom torchmetrics import Accuracy\n\n# Reproducibility\ndef seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nseed_everything(42)\n\n# Device setup — use both T4s with DataParallel\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\nprint(f'GPU count: {torch.cuda.device_count()}')\nif torch.cuda.is_available():\n    for i in range(torch.cuda.device_count()):\n        print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:46:53.966937Z","iopub.execute_input":"2026-02-25T16:46:53.967311Z","iopub.status.idle":"2026-02-25T16:46:53.978263Z","shell.execute_reply.started":"2026-02-25T16:46:53.967275Z","shell.execute_reply":"2026-02-25T16:46:53.977494Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU count: 2\n  GPU 0: Tesla T4\n  GPU 1: Tesla T4\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 1. Configuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    # ─── Paths ────────────────────────────────────────────────────\n    # UPDATE THESE to match your Kaggle dataset paths\n    S2A_ROOT       = '/kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026p2/s2a'  # unlabeled SSL data\n    LABELED_ROOT   = '/kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026p2/ICPR02'  # labeled data\n    OUTPUT_DIR     = '/kaggle/working/'\n\n    # ─── Data ─────────────────────────────────────────────────────\n    NUM_CLASSES    = 4          # Aphid, Rust, RPH, Blast\n    IMG_SIZE       = 64         # patch size (adjust if your tiles are larger)\n    IN_CHANNELS    = 16         # 12 Sentinel-2 bands + 4 vegetation indices\n    BANDS          = ['B1','B2','B3','B4','B5','B6','B7','B8','B8A','B9','B11','B12']\n    CLASS_NAMES    = ['Aphid', 'Rust', 'RPH', 'Blast']\n    CLASS2IDX      = {c: i for i, c in enumerate(CLASS_NAMES)}\n\n    # ─── SSL Pretraining ──────────────────────────────────────────\n    SSL_EPOCHS     = 50         # increase to 100-200 if time permits\n    SSL_BATCH_SIZE = 64         # per GPU; effective batch = 128 with 2x T4\n    SSL_LR         = 1.5e-4\n    SSL_MASK_RATIO = 0.75       # MAE masking ratio\n    SSL_PATCH_SIZE = 8          # ViT patch size (8x8 for 64x64 images)\n\n    # ─── Fine-tuning ──────────────────────────────────────────────\n    FT_EPOCHS      = 40\n    FT_BATCH_SIZE  = 32\n    FT_LR          = 5e-5\n    LR_DECAY       = 0.75       # layer-wise LR decay factor\n    WEIGHT_DECAY   = 0.05\n    WARMUP_EPOCHS  = 5\n    LABEL_SMOOTH   = 0.1\n    N_FOLDS        = 5\n\n    # ─── Ensemble ─────────────────────────────────────────────────\n    # Weights: [ViT-MAE, Swin-Tiny, ConvNeXt-Small]\n    ENSEMBLE_WEIGHTS = [0.45, 0.30, 0.25]\n    TTA_AUGS         = 8        # number of TTA augmentations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:46:53.979215Z","iopub.execute_input":"2026-02-25T16:46:53.979673Z","iopub.status.idle":"2026-02-25T16:46:53.995561Z","shell.execute_reply.started":"2026-02-25T16:46:53.979652Z","shell.execute_reply":"2026-02-25T16:46:53.994785Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"## 2. Data Loading & Preprocessing","metadata":{}},{"cell_type":"code","source":"# ─── Band Loading Utilities ───────────────────────────────────────────────────\n\ndef load_sentinel2_patch(folder_path, img_size=CFG.IMG_SIZE):\n    \"\"\"\n    Load all 12 Sentinel-2 bands from a folder of .tif files.\n    Returns numpy array of shape (12, H, W), normalized to [0, 1].\n    \"\"\"\n    bands = []\n    for band_name in CFG.BANDS:\n        tif_path = os.path.join(folder_path, f'{band_name}.tif')\n        if not os.path.exists(tif_path):\n            # Try alternative naming\n            candidates = glob.glob(os.path.join(folder_path, f'*{band_name}*.tif'))\n            tif_path = candidates[0] if candidates else None\n        \n        if tif_path and os.path.exists(tif_path):\n            with rasterio.open(tif_path) as src:\n                arr = src.read(1).astype(np.float32)\n        else:\n            arr = np.zeros((img_size, img_size), dtype=np.float32)\n        \n        # Resize if needed\n        if arr.shape != (img_size, img_size):\n            from PIL import Image\n            arr = np.array(Image.fromarray(arr).resize((img_size, img_size), Image.BILINEAR))\n        \n        bands.append(arr)\n    \n    img = np.stack(bands, axis=0)  # (12, H, W)\n    \n    # Normalize each band to [0, 1] using percentile clipping\n    for i in range(img.shape[0]):\n        p2, p98 = np.percentile(img[i], [2, 98])\n        img[i] = np.clip(img[i], p2, p98)\n        if p98 > p2:\n            img[i] = (img[i] - p2) / (p98 - p2)\n        else:\n            img[i] = np.zeros_like(img[i])\n    \n    return img\n\n\ndef compute_vegetation_indices(img):\n    \"\"\"\n    Compute 4 vegetation indices from 12-band Sentinel-2 image.\n    img: numpy (12, H, W) normalized [0,1]\n    Returns: (16, H, W) — original 12 bands + 4 indices\n    \n    Band mapping (0-indexed):\n      0:B1, 1:B2, 2:B3, 3:B4, 4:B5, 5:B6, 6:B7,\n      7:B8, 8:B8A, 9:B9, 10:B11, 11:B12\n    \"\"\"\n    eps = 1e-6\n    B2, B4, B5, B6 = img[1], img[3], img[4], img[5]\n    B8, B8A, B11, B12 = img[7], img[8], img[10], img[11]\n\n    NDVI  = (B8  - B4)  / (B8  + B4  + eps)          # vegetation health\n    NDRE  = (B8A - B5)  / (B8A + B5  + eps)          # red-edge disease\n    PSRI  = (B4  - B2)  / (B6  + eps)                # plant senescence (rust)\n    SWIR  = B11         / (B12 + eps)                 # moisture stress\n\n    indices = np.stack([NDVI, NDRE, PSRI, SWIR], axis=0)  # (4, H, W)\n    \n    # Normalize indices to [0, 1]\n    for i in range(4):\n        mn, mx = indices[i].min(), indices[i].max()\n        if mx > mn:\n            indices[i] = (indices[i] - mn) / (mx - mn)\n    \n    return np.concatenate([img, indices], axis=0)  # (16, H, W)\n\n\nprint('Data utilities defined.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:46:53.996671Z","iopub.execute_input":"2026-02-25T16:46:53.996951Z","iopub.status.idle":"2026-02-25T16:46:54.009917Z","shell.execute_reply.started":"2026-02-25T16:46:53.996919Z","shell.execute_reply":"2026-02-25T16:46:54.009394Z"}},"outputs":[{"name":"stdout","text":"Data utilities defined.\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# ─── FIXED Dataset Discovery ─────────────────────────────────────────────────\n\n# Override paths — data lives inside 'kaggle' subfolder\nCFG.LABELED_ROOT = '/kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026p2/ICPR02/kaggle'\nCFG.S2A_ROOT     = '/kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026p2/s2a'\n\n# Update class names to match exact folder names on disk\nCFG.CLASS_NAMES = ['Aphid', 'Blast', 'RPH', 'Rust']\nCFG.CLASS2IDX   = {c: i for i, c in enumerate(CFG.CLASS_NAMES)}\nCFG.NUM_CLASSES = 4\n\ndef discover_labeled_data(root):\n    \"\"\"\n    Structure: root/ClassName/sample_hash/ containing B1.tif ... B12.tif\n    \"\"\"\n    records = []\n    for cls in CFG.CLASS_NAMES:\n        cls_dir = os.path.join(root, cls)\n        if not os.path.isdir(cls_dir):\n            print(f'  WARNING: class folder not found: {cls_dir}')\n            continue\n        samples = [s for s in os.listdir(cls_dir)\n                   if os.path.isdir(os.path.join(cls_dir, s))]\n        for sample in samples:\n            records.append({\n                'path':      os.path.join(cls_dir, sample),\n                'label':     cls,\n                'label_idx': CFG.CLASS2IDX[cls]\n            })\n        print(f'  {cls:10s}: {len(samples)} samples')\n\n    df = pd.DataFrame(records)\n    print(f'\\nTotal labeled samples: {len(df)}')\n    return df\n\n\ndef discover_unlabeled_data(root, max_samples=5000):\n    \"\"\"\n    Find unlabeled Sentinel-2 folders for SSL pretraining.\n    \"\"\"\n    folders = []\n    for dirpath, dirnames, filenames in os.walk(root):\n        if any(f.endswith('.tif') for f in filenames):\n            folders.append(dirpath)\n        if len(folders) >= max_samples:\n            break\n    print(f'Found {len(folders)} unlabeled S2 folders')\n    return folders\n\n\ndef discover_test_data(root):\n    \"\"\"\n    Test set: root/evaluation/sample_hash/\n    \"\"\"\n    eval_dir = os.path.join(root, 'evaluation')\n    if not os.path.isdir(eval_dir):\n        print(f'WARNING: evaluation folder not found at {eval_dir}')\n        return []\n    folders = [os.path.join(eval_dir, s)\n               for s in sorted(os.listdir(eval_dir))\n               if os.path.isdir(os.path.join(eval_dir, s))]\n    print(f'Test samples: {len(folders)}')\n    return folders\n\n\n# Discover data\nlabeled_df        = discover_labeled_data(CFG.LABELED_ROOT)\nunlabeled_folders = discover_unlabeled_data(CFG.S2A_ROOT)\ntest_folders      = discover_test_data(CFG.LABELED_ROOT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:46:54.011918Z","iopub.execute_input":"2026-02-25T16:46:54.012183Z","iopub.status.idle":"2026-02-25T16:46:54.664712Z","shell.execute_reply.started":"2026-02-25T16:46:54.012163Z","shell.execute_reply":"2026-02-25T16:46:54.663987Z"}},"outputs":[{"name":"stdout","text":"  Aphid     : 290 samples\n  Blast     : 75 samples\n  RPH       : 495 samples\n  Rust      : 40 samples\n\nTotal labeled samples: 900\nFound 0 unlabeled S2 folders\nTest samples: 40\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# ─── PyTorch Datasets ────────────────────────────────────────────────────────\n\nclass S2UnlabeledDataset(Dataset):\n    \"\"\"Unlabeled Sentinel-2 dataset for SSL pretraining.\"\"\"\n    \n    def __init__(self, folders, img_size=CFG.IMG_SIZE):\n        self.folders = folders\n        self.img_size = img_size\n    \n    def __len__(self):\n        return len(self.folders)\n    \n    def __getitem__(self, idx):\n        try:\n            img = load_sentinel2_patch(self.folders[idx], self.img_size)\n            img = compute_vegetation_indices(img)   # (16, H, W)\n            return torch.FloatTensor(img)\n        except Exception as e:\n            # Return zeros on error — SSL can tolerate occasional bad samples\n            return torch.zeros(CFG.IN_CHANNELS, self.img_size, self.img_size)\n\n\nclass S2LabeledDataset(Dataset):\n    \"\"\"Labeled dataset for fine-tuning.\"\"\"\n    \n    def __init__(self, df, img_size=CFG.IMG_SIZE, augment=True):\n        self.df = df.reset_index(drop=True)\n        self.img_size = img_size\n        self.augment = augment\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def _augment(self, img):\n        \"\"\"Spectral + spatial augmentations.\"\"\"\n        # Random horizontal flip\n        if random.random() > 0.5:\n            img = np.flip(img, axis=2).copy()\n        # Random vertical flip\n        if random.random() > 0.5:\n            img = np.flip(img, axis=1).copy()\n        # Random 90° rotation\n        k = random.randint(0, 3)\n        img = np.rot90(img, k=k, axes=(1, 2)).copy()\n        # Spectral dropout: randomly zero out 1-2 bands\n        if random.random() > 0.7:\n            n_drop = random.randint(1, 2)\n            drop_idx = random.sample(range(12), n_drop)  # only drop raw bands, not indices\n            for d in drop_idx:\n                img[d] = 0.0\n        return img\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        try:\n            img = load_sentinel2_patch(row['path'], self.img_size)\n            img = compute_vegetation_indices(img)  # (16, H, W)\n            if self.augment:\n                img = self._augment(img)\n        except Exception:\n            img = np.zeros((CFG.IN_CHANNELS, self.img_size, self.img_size), dtype=np.float32)\n        \n        return torch.FloatTensor(img), torch.tensor(row['label_idx'], dtype=torch.long)\n\n\nprint('Datasets defined.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:46:54.665673Z","iopub.execute_input":"2026-02-25T16:46:54.665919Z","iopub.status.idle":"2026-02-25T16:46:54.677280Z","shell.execute_reply.started":"2026-02-25T16:46:54.665895Z","shell.execute_reply":"2026-02-25T16:46:54.676635Z"}},"outputs":[{"name":"stdout","text":"Datasets defined.\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"## 3. Stage 1: MAE Self-Supervised Pretraining","metadata":{}},{"cell_type":"code","source":"# ─── MAE Model ───────────────────────────────────────────────────────────────\n# Masked Autoencoder adapted for 16-channel Sentinel-2 input\n\nclass PatchEmbed(nn.Module):\n    \"\"\"2D image to patch embeddings — supports arbitrary number of input channels.\"\"\"\n    \n    def __init__(self, img_size=64, patch_size=8, in_chans=16, embed_dim=384):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n    \n    def forward(self, x):\n        # x: (B, C, H, W) → (B, n_patches, embed_dim)\n        x = self.proj(x)                    # (B, embed_dim, H/P, W/P)\n        x = x.flatten(2).transpose(1, 2)    # (B, n_patches, embed_dim)\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Standard Transformer block with pre-norm.\"\"\"\n    \n    def __init__(self, dim, n_heads, mlp_ratio=4.0, dropout=0.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn  = nn.MultiheadAttention(dim, n_heads, dropout=dropout, batch_first=True)\n        self.norm2 = nn.LayerNorm(dim)\n        mlp_dim    = int(dim * mlp_ratio)\n        self.mlp   = nn.Sequential(\n            nn.Linear(dim, mlp_dim), nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_dim, dim), nn.Dropout(dropout)\n        )\n    \n    def forward(self, x):\n        x_norm = self.norm1(x)\n        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n        x = x + attn_out\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n\nclass MAEEncoder(nn.Module):\n    \"\"\"ViT-Small encoder (memory-efficient for T4).\"\"\"\n    \n    def __init__(self,\n                 img_size=CFG.IMG_SIZE,\n                 patch_size=CFG.SSL_PATCH_SIZE,\n                 in_chans=CFG.IN_CHANNELS,\n                 embed_dim=384,           # ViT-Small dimension\n                 depth=12,\n                 n_heads=6):\n        super().__init__()\n        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n        self.n_patches   = self.patch_embed.n_patches\n        self.embed_dim   = embed_dim\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.n_patches + 1, embed_dim))\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, n_heads) for _ in range(depth)\n        ])\n        self.norm = nn.LayerNorm(embed_dim)\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None: nn.init.zeros_(m.bias)\n    \n    def random_masking(self, x, mask_ratio):\n        \"\"\"\n        Random token masking for MAE.\n        Returns: x_masked, mask, ids_restore\n        \"\"\"\n        B, N, D = x.shape\n        keep = int(N * (1 - mask_ratio))\n        \n        noise = torch.rand(B, N, device=x.device)\n        ids_shuffle = torch.argsort(noise, dim=1)\n        ids_restore = torch.argsort(ids_shuffle, dim=1)\n        \n        ids_keep    = ids_shuffle[:, :keep]\n        x_masked    = torch.gather(x, 1, ids_keep.unsqueeze(-1).expand(-1, -1, D))\n        \n        mask = torch.ones(B, N, device=x.device)\n        mask[:, :keep] = 0\n        mask = torch.gather(mask, 1, ids_restore)\n        \n        return x_masked, mask, ids_restore\n    \n    def forward(self, x, mask_ratio=0.0):\n        x = self.patch_embed(x)\n        x = x + self.pos_embed[:, 1:, :]  # add positional embedding (no cls)\n        \n        mask, ids_restore = None, None\n        if mask_ratio > 0:\n            x, mask, ids_restore = self.random_masking(x, mask_ratio)\n        \n        cls = self.cls_token.expand(x.shape[0], -1, -1) + self.pos_embed[:, :1, :]\n        x   = torch.cat([cls, x], dim=1)\n        \n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        \n        return x, mask, ids_restore\n\n\nclass MAEDecoder(nn.Module):\n    \"\"\"Lightweight MAE decoder — only used during pretraining.\"\"\"\n    \n    def __init__(self, n_patches, encoder_dim=384, decoder_dim=192,\n                 depth=4, n_heads=4, patch_size=8, in_chans=16):\n        super().__init__()\n        self.n_patches    = n_patches\n        self.patch_size   = patch_size\n        self.in_chans     = in_chans\n        \n        self.proj         = nn.Linear(encoder_dim, decoder_dim)\n        self.mask_token   = nn.Parameter(torch.zeros(1, 1, decoder_dim))\n        self.pos_embed    = nn.Parameter(torch.zeros(1, n_patches + 1, decoder_dim))\n        \n        self.blocks       = nn.ModuleList([\n            TransformerBlock(decoder_dim, n_heads) for _ in range(depth)\n        ])\n        self.norm         = nn.LayerNorm(decoder_dim)\n        self.head         = nn.Linear(decoder_dim, patch_size * patch_size * in_chans)\n        \n        nn.init.trunc_normal_(self.mask_token, std=0.02)\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n    \n    def forward(self, x, ids_restore):\n        x = self.proj(x)\n        B, n_keep_plus1, D = x.shape\n        n_keep = n_keep_plus1 - 1  # subtract cls token\n        \n        # Expand mask tokens\n        mask_tokens = self.mask_token.expand(B, self.n_patches - n_keep, -1)\n        x_no_cls    = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # (B, N, D)\n        x_no_cls    = torch.gather(x_no_cls, 1,\n                          ids_restore.unsqueeze(-1).expand(-1, -1, D))\n        x           = torch.cat([x[:, :1, :], x_no_cls], dim=1)   # add cls back\n        x           = x + self.pos_embed\n        \n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        x = self.head(x[:, 1:, :])  # remove cls, predict patches\n        return x\n\n\nclass MAE(nn.Module):\n    \"\"\"Full MAE model for pretraining.\"\"\"\n    \n    def __init__(self, mask_ratio=CFG.SSL_MASK_RATIO):\n        super().__init__()\n        self.mask_ratio = mask_ratio\n        self.encoder    = MAEEncoder()\n        self.decoder    = MAEDecoder(\n            n_patches   = self.encoder.n_patches,\n            encoder_dim = self.encoder.embed_dim\n        )\n    \n    def patchify(self, x):\n        \"\"\"x: (B, C, H, W) → (B, n_patches, patch_size²×C)\"\"\"\n        P = CFG.SSL_PATCH_SIZE\n        B, C, H, W = x.shape\n        x = x.reshape(B, C, H//P, P, W//P, P)\n        x = x.permute(0, 2, 4, 3, 5, 1).reshape(B, (H//P)*(W//P), P*P*C)\n        return x\n    \n    def forward(self, x):\n        target = self.patchify(x)\n        \n        latent, mask, ids_restore = self.encoder(x, mask_ratio=self.mask_ratio)\n        pred = self.decoder(latent, ids_restore)\n        \n        # MSE loss only on masked patches\n        loss = ((pred - target) ** 2)\n        loss = loss.mean(dim=-1)          # (B, N)\n        loss = (loss * mask).sum() / (mask.sum() + 1e-6)\n        return loss\n\n\nprint('MAE model defined.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:46:54.678297Z","iopub.execute_input":"2026-02-25T16:46:54.679152Z","iopub.status.idle":"2026-02-25T16:46:54.707409Z","shell.execute_reply.started":"2026-02-25T16:46:54.679130Z","shell.execute_reply":"2026-02-25T16:46:54.706520Z"}},"outputs":[{"name":"stdout","text":"MAE model defined.\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# ─── SSL Pretraining Loop ─────────────────────────────────────────────────────\n\ndef pretrain_mae(folders, save_path=os.path.join(CFG.OUTPUT_DIR, 'mae_pretrained.pth')):\n    \"\"\"\n    Pretrain MAE on unlabeled Sentinel-2 data.\n    Returns path to saved encoder weights.\n    \"\"\"\n    print('\\n' + '='*60)\n    print('STAGE 1: MAE Self-Supervised Pretraining')\n    print('='*60)\n    \n    # Dataset\n    dataset = S2UnlabeledDataset(folders)\n    loader  = DataLoader(dataset, batch_size=CFG.SSL_BATCH_SIZE,\n                         shuffle=True, num_workers=4,\n                         pin_memory=True, drop_last=True)\n    print(f'SSL dataset size: {len(dataset)}')\n    print(f'Steps per epoch:  {len(loader)}')\n    \n    # Model\n    model = MAE()\n    if torch.cuda.device_count() > 1:\n        print(f'Using {torch.cuda.device_count()} GPUs for pretraining')\n        model = nn.DataParallel(model)\n    model = model.to(device)\n    \n    # Optimizer & Scheduler\n    optimizer = torch.optim.AdamW(\n        model.parameters(), lr=CFG.SSL_LR,\n        betas=(0.9, 0.95), weight_decay=0.05\n    )\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=CFG.SSL_EPOCHS\n    )\n    scaler = torch.cuda.amp.GradScaler()  # mixed precision\n    \n    best_loss = float('inf')\n    for epoch in range(1, CFG.SSL_EPOCHS + 1):\n        model.train()\n        total_loss = 0\n        \n        pbar = tqdm(loader, desc=f'SSL Epoch {epoch}/{CFG.SSL_EPOCHS}')\n        for imgs in pbar:\n            imgs = imgs.to(device, non_blocking=True)\n            \n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast():\n                loss = model(imgs)\n                if isinstance(loss, torch.Tensor) is False:\n                    loss = loss.mean()  # DataParallel returns tuple\n            \n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            \n            total_loss += loss.item()\n            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        scheduler.step()\n        avg_loss = total_loss / len(loader)\n        print(f'Epoch {epoch:3d} | Loss: {avg_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}')\n        \n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            # Save encoder only (decoder not needed for fine-tuning)\n            enc_state = model.module.encoder.state_dict() if hasattr(model, 'module') \\\n                        else model.encoder.state_dict()\n            torch.save(enc_state, save_path)\n            print(f'  ✓ Saved encoder (loss={best_loss:.4f})')\n    \n    print(f'\\nPretraining complete. Best loss: {best_loss:.4f}')\n    return save_path\n\n\n# Run pretraining (skip if already done)\nssl_weights_path = os.path.join(CFG.OUTPUT_DIR, 'mae_pretrained.pth')\nif not os.path.exists(ssl_weights_path) and len(unlabeled_folders) > 0:\n    ssl_weights_path = pretrain_mae(unlabeled_folders, ssl_weights_path)\nelse:\n    print(f'SSL weights found / no unlabeled data: {ssl_weights_path}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:46:54.708569Z","iopub.execute_input":"2026-02-25T16:46:54.708856Z","iopub.status.idle":"2026-02-25T16:46:54.725777Z","shell.execute_reply.started":"2026-02-25T16:46:54.708829Z","shell.execute_reply":"2026-02-25T16:46:54.724970Z"}},"outputs":[{"name":"stdout","text":"SSL weights found / no unlabeled data: /kaggle/working/mae_pretrained.pth\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"## 4. Stage 2: Fine-Tuning Three Models","metadata":{}},{"cell_type":"code","source":"# ─── Model 1: MAE ViT-Base (SSL pretrained) ───────────────────────────────────\nclass ViTClassifier(nn.Module):\n    def __init__(self, encoder, num_classes=CFG.NUM_CLASSES, dropout=0.2):\n        super().__init__()\n        self.encoder = encoder\n        dim = encoder.embed_dim\n        self.head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Dropout(dropout),\n            nn.Linear(dim, 256),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, x):\n        out, _, _ = self.encoder(x, mask_ratio=0.0)\n        cls = out[:, 0, :]\n        return self.head(cls)\n\n\ndef build_vit_model(ssl_weights_path):\n    encoder = MAEEncoder()\n    if os.path.exists(ssl_weights_path):\n        state = torch.load(ssl_weights_path, map_location='cpu')\n        encoder.load_state_dict(state, strict=False)\n        print('Loaded MAE pretrained encoder weights')\n    else:\n        print('WARNING: No SSL weights found, training ViT from scratch')\n    return ViTClassifier(encoder)\n\n\n# ─── Model 2: Swin-Tiny (fixed for 64x64 input + 16 channels) ────────────────\ndef build_swin_model(num_classes=CFG.NUM_CLASSES):\n    model = timm.create_model(\n        'swin_tiny_patch4_window7_224',\n        pretrained=True,\n        num_classes=num_classes,\n        img_size=CFG.IMG_SIZE          # ← KEY FIX: tell timm to use 64x64\n    )\n\n    old_proj   = model.patch_embed.proj\n    old_weight = old_proj.weight.data  # (96, 3, 4, 4)\n\n    new_proj = nn.Conv2d(\n        CFG.IN_CHANNELS, old_proj.out_channels,\n        kernel_size=old_proj.kernel_size,\n        stride=old_proj.stride,\n        padding=old_proj.padding,\n        bias=False\n    )\n    with torch.no_grad():\n        repeats    = CFG.IN_CHANNELS // 3 + 1\n        new_weight = old_weight.repeat(1, repeats, 1, 1)[:, :CFG.IN_CHANNELS, :, :]\n        new_weight = new_weight * (3.0 / CFG.IN_CHANNELS)\n        new_proj.weight.data = new_weight\n\n    model.patch_embed.proj = new_proj\n    print(f'Swin-Tiny: adapted to {CFG.IN_CHANNELS}-channel {CFG.IMG_SIZE}x{CFG.IMG_SIZE} input')\n    return model\n\n\n# ─── Model 3: ConvNeXt-Small (16 channels, no size restriction) ───────────────\ndef build_convnext_model(num_classes=CFG.NUM_CLASSES):\n    model = timm.create_model(\n        'convnext_small.fb_in22k',\n        pretrained=True,\n        num_classes=num_classes\n    )\n\n    old_conv   = model.stem[0]\n    old_weight = old_conv.weight.data  # (96, 3, 4, 4)\n\n    new_conv = nn.Conv2d(\n        CFG.IN_CHANNELS, old_conv.out_channels,\n        kernel_size=old_conv.kernel_size,\n        stride=old_conv.stride,\n        padding=old_conv.padding,\n        bias=False\n    )\n    with torch.no_grad():\n        repeats    = CFG.IN_CHANNELS // 3 + 1\n        new_weight = old_weight.repeat(1, repeats, 1, 1)[:, :CFG.IN_CHANNELS, :, :]\n        new_weight = new_weight * (3.0 / CFG.IN_CHANNELS)\n        new_conv.weight.data = new_weight\n\n    model.stem[0] = new_conv\n    print(f'ConvNeXt-Small: adapted to {CFG.IN_CHANNELS}-channel input')\n    return model\n\n\nprint('All model builders defined.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:46:54.726646Z","iopub.execute_input":"2026-02-25T16:46:54.726952Z","iopub.status.idle":"2026-02-25T16:46:54.743640Z","shell.execute_reply.started":"2026-02-25T16:46:54.726929Z","shell.execute_reply":"2026-02-25T16:46:54.742822Z"}},"outputs":[{"name":"stdout","text":"All model builders defined.\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# ─── Training Utilities ───────────────────────────────────────────────────────\n\nclass FocalLoss(nn.Module):\n    \"\"\"Focal loss for class-imbalanced datasets.\"\"\"\n    \n    def __init__(self, gamma=2.0, label_smooth=CFG.LABEL_SMOOTH):\n        super().__init__()\n        self.gamma        = gamma\n        self.label_smooth = label_smooth\n    \n    def forward(self, logits, targets):\n        B, C = logits.shape\n        # Label smoothing\n        with torch.no_grad():\n            soft_targets = torch.full_like(logits, self.label_smooth / (C - 1))\n            soft_targets.scatter_(1, targets.unsqueeze(1), 1 - self.label_smooth)\n        \n        log_probs = F.log_softmax(logits, dim=-1)\n        probs     = log_probs.exp()\n        focal_w   = (1 - probs) ** self.gamma\n        loss      = -(soft_targets * focal_w * log_probs).sum(dim=-1)\n        return loss.mean()\n\n\ndef mixup_data(x, y, alpha=0.2):\n    \"\"\"MixUp augmentation.\"\"\"\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1.0\n    idx = torch.randperm(x.size(0), device=x.device)\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    return mixed_x, y, y[idx], lam\n\n\ndef get_layer_wise_optimizer(model, base_lr, decay=CFG.LR_DECAY):\n    \"\"\"\n    Layer-wise learning rate decay:\n    - Deepest (earliest) layers get lower LR\n    - Final classification head gets full LR\n    \"\"\"\n    param_groups = []\n    named_params = list(model.named_parameters())\n    n_layers = len(named_params)\n    \n    for i, (name, param) in enumerate(named_params):\n        layer_scale = decay ** (n_layers - i - 1)\n        # Head layers get full LR\n        if any(k in name for k in ['head', 'classifier', 'fc']):\n            layer_scale = 1.0\n        param_groups.append({'params': [param], 'lr': base_lr * layer_scale})\n    \n    return torch.optim.AdamW(param_groups, weight_decay=CFG.WEIGHT_DECAY)\n\n\ndef get_cosine_schedule_with_warmup(optimizer, warmup_epochs, total_epochs):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return epoch / max(1, warmup_epochs)\n        progress = (epoch - warmup_epochs) / max(1, total_epochs - warmup_epochs)\n        return 0.5 * (1 + np.cos(np.pi * progress))\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n\nprint('Training utilities defined.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:46:54.744518Z","iopub.execute_input":"2026-02-25T16:46:54.744825Z","iopub.status.idle":"2026-02-25T16:46:54.769613Z","shell.execute_reply.started":"2026-02-25T16:46:54.744792Z","shell.execute_reply":"2026-02-25T16:46:54.768928Z"}},"outputs":[{"name":"stdout","text":"Training utilities defined.\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# ─── Fine-Tuning Loop ─────────────────────────────────────────────────────────\n\ndef train_one_epoch(model, loader, optimizer, criterion, scaler, epoch):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n    \n    for imgs, labels in tqdm(loader, desc=f'  Train epoch {epoch}', leave=False):\n        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n        \n        # MixUp\n        imgs, y_a, y_b, lam = mixup_data(imgs, labels)\n        \n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():\n            logits = model(imgs)\n            loss   = lam * criterion(logits, y_a) + (1 - lam) * criterion(logits, y_b)\n        \n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        \n        total_loss += loss.item()\n        preds = logits.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total   += labels.size(0)\n    \n    return total_loss / len(loader), correct / total\n\n\n@torch.no_grad()\ndef evaluate(model, loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    \n    for imgs, labels in tqdm(loader, desc='  Validating', leave=False):\n        imgs = imgs.to(device, non_blocking=True)\n        with torch.cuda.amp.autocast():\n            logits = model(imgs)\n        preds = logits.argmax(dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(labels.numpy())\n    \n    acc = accuracy_score(all_labels, all_preds)\n    f1  = f1_score(all_labels, all_preds, average='macro')\n    return acc, f1, all_preds, all_labels\n\n\ndef finetune_model(model, train_df, val_df, model_name, save_path):\n    \"\"\"Fine-tune a model on labeled data.\"\"\"\n    print(f'\\nFine-tuning: {model_name}')\n    \n    # Datasets\n    train_ds = S2LabeledDataset(train_df, augment=True)\n    val_ds   = S2LabeledDataset(val_df,   augment=False)\n    \n    # Class-balanced sampler\n    class_counts = train_df['label_idx'].value_counts().sort_index().values\n    weights      = 1.0 / class_counts[train_df['label_idx'].values]\n    sampler      = WeightedRandomSampler(weights, len(weights), replacement=True)\n    \n    train_loader = DataLoader(train_ds, batch_size=CFG.FT_BATCH_SIZE,\n                              sampler=sampler, num_workers=4, pin_memory=True)\n    val_loader   = DataLoader(val_ds,   batch_size=CFG.FT_BATCH_SIZE * 2,\n                              shuffle=False, num_workers=4, pin_memory=True)\n    model = model.to(device)\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n    \n    \n    optimizer = get_layer_wise_optimizer(model, CFG.FT_LR)\n    scheduler = get_cosine_schedule_with_warmup(optimizer, CFG.WARMUP_EPOCHS, CFG.FT_EPOCHS)\n    criterion = FocalLoss()\n    scaler    = torch.cuda.amp.GradScaler()\n    \n    best_f1 = 0\n    history = []\n    \n    for epoch in range(1, CFG.FT_EPOCHS + 1):\n        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, scaler, epoch)\n        val_acc, val_f1, _, _ = evaluate(model, val_loader)\n        scheduler.step()\n        \n        history.append({'epoch': epoch, 'tr_loss': tr_loss, 'tr_acc': tr_acc,\n                         'val_acc': val_acc, 'val_f1': val_f1})\n        \n        print(f'  Epoch {epoch:3d} | tr_loss={tr_loss:.4f} tr_acc={tr_acc:.4f} '\n              f'val_acc={val_acc:.4f} val_f1={val_f1:.4f}', end='')\n        \n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            state = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n            torch.save(state, save_path)\n            print(' ✓ saved', end='')\n        print()\n    \n    print(f'  Best val F1: {best_f1:.4f}')\n    return best_f1, history\n\n\nprint('Fine-tuning loop defined.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:46:54.770740Z","iopub.execute_input":"2026-02-25T16:46:54.771506Z","iopub.status.idle":"2026-02-25T16:46:54.789773Z","shell.execute_reply.started":"2026-02-25T16:46:54.771483Z","shell.execute_reply":"2026-02-25T16:46:54.788959Z"}},"outputs":[{"name":"stdout","text":"Fine-tuning loop defined.\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# ─── DEBUG CELL — run this and paste the output ───────────────────────────────\nimport os, glob\n\nLABELED_ROOT = '/kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026p2/ICPR02'\n\nprint(\"=== TOP-LEVEL CONTENTS ===\")\nif os.path.exists(LABELED_ROOT):\n    for item in sorted(os.listdir(LABELED_ROOT))[:30]:\n        full = os.path.join(LABELED_ROOT, item)\n        kind = 'DIR' if os.path.isdir(full) else 'FILE'\n        print(f\"  [{kind}] {item}\")\nelse:\n    print(f\"PATH DOES NOT EXIST: {LABELED_ROOT}\")\n    # Try to find it\n    print(\"\\n=== Searching for ICPR02 ===\")\n    for root, dirs, files in os.walk('/kaggle/input'):\n        for d in dirs:\n            if 'ICPR' in d or 'icpr' in d.lower():\n                print(f\"  Found: {os.path.join(root, d)}\")\n\nprint(\"\\n=== FIRST SUBFOLDER CONTENTS ===\")\nif os.path.exists(LABELED_ROOT):\n    items = sorted(os.listdir(LABELED_ROOT))\n    if items:\n        first = os.path.join(LABELED_ROOT, items[0])\n        if os.path.isdir(first):\n            sub = sorted(os.listdir(first))[:20]\n            print(f\"Inside '{items[0]}':\")\n            for s in sub:\n                full2 = os.path.join(first, s)\n                kind  = 'DIR' if os.path.isdir(full2) else 'FILE'\n                print(f\"  [{kind}] {s}\")\n            # Go one level deeper if subfolders exist\n            subdirs = [s for s in sub if os.path.isdir(os.path.join(first, s))]\n            if subdirs:\n                deeper = os.path.join(first, subdirs[0])\n                print(f\"\\n  Inside '{subdirs[0]}':\")\n                for x in sorted(os.listdir(deeper))[:20]:\n                    print(f\"    {x}\")\n\nprint(\"\\n=== JSON FILES (first 10) ===\")\njsons = glob.glob(os.path.join(LABELED_ROOT, '**/*.json'), recursive=True)\nprint(f\"Total JSON files: {len(jsons)}\")\nfor j in jsons[:10]:\n    print(f\"  {j}\")\n    with open(j) as f:\n        import json\n        print(f\"    contents: {json.load(f)}\")\n\nprint(\"\\n=== TIF FILES sample (first 5) ===\")\ntifs = glob.glob(os.path.join(LABELED_ROOT, '**/*.tif'), recursive=True)\nprint(f\"Total TIF files: {len(tifs)}\")\nfor t in tifs[:5]:\n    print(f\"  {t}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:46:54.790785Z","iopub.execute_input":"2026-02-25T16:46:54.791234Z","iopub.status.idle":"2026-02-25T16:46:56.991638Z","shell.execute_reply.started":"2026-02-25T16:46:54.791184Z","shell.execute_reply":"2026-02-25T16:46:56.991020Z"}},"outputs":[{"name":"stdout","text":"=== TOP-LEVEL CONTENTS ===\n  [DIR] kaggle\n\n=== FIRST SUBFOLDER CONTENTS ===\nInside 'kaggle':\n  [DIR] Aphid\n  [DIR] Blast\n  [DIR] RPH\n  [DIR] Rust\n  [DIR] evaluation\n\n  Inside 'Aphid':\n    0041231a3f6f4fa9b07a04234cef4627\n    00e6adf1215344a0aa3d396aa50eff0c\n    018a98b4251441f0b9e73b9d286541f2\n    0299e35c64b74d3d896f7a22227cde31\n    035d3057f7af4f1c9b47e2a325b71be2\n    03b38c96e4d8428cbd726ebf8e9a368a\n    044515d231cc403e80b6a1e2b45862b7\n    05688b9f038941e9b291fab9aaf140ee\n    05690e774e08431896edaded9196e3e5\n    05915a998f9b4a7e923f6ee96643f2c2\n    066c5e545c9d4ddd9a3c637ca0cccc17\n    0801155fd3a749a0932140c1b0b7ab42\n    083263e8f4774f95b02656705c9aa298\n    08924b82960441eb82444741c8102c93\n    09210ee84323441db5e4dd5967c2a3b6\n    09bdf2946f6d466c8b8a7ecd5ce3e231\n    09c6fee5137f4613bf3591055ac0ce9e\n    09d84961c05548c386ef69eb2d008a7a\n    0b5c38795f354b82ad57aede75610004\n    0d86f8fa182043e4ad8df395e9079bcf\n\n=== JSON FILES (first 10) ===\nTotal JSON files: 0\n\n=== TIF FILES sample (first 5) ===\nTotal TIF files: 11280\n  /kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026p2/ICPR02/kaggle/RPH/e5661c0424fa494b87850135cf10e34a/B7.tif\n  /kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026p2/ICPR02/kaggle/RPH/e5661c0424fa494b87850135cf10e34a/B2.tif\n  /kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026p2/ICPR02/kaggle/RPH/e5661c0424fa494b87850135cf10e34a/B3.tif\n  /kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026p2/ICPR02/kaggle/RPH/e5661c0424fa494b87850135cf10e34a/B11.tif\n  /kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026p2/ICPR02/kaggle/RPH/e5661c0424fa494b87850135cf10e34a/B5.tif\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# ─── Run Fine-Tuning for All 3 Models ─────────────────────────────────────────\n\n# 5-fold CV — we use fold 0 for simplicity; run all folds for full ensemble diversity\nskf = StratifiedKFold(n_splits=CFG.N_FOLDS, shuffle=True, random_state=42)\nsplits = list(skf.split(labeled_df, labeled_df['label_idx']))\n\n# Use fold 0\ntrain_idx, val_idx = splits[0]\ntrain_df = labeled_df.iloc[train_idx]\nval_df   = labeled_df.iloc[val_idx]\nprint(f'Train: {len(train_df)} | Val: {len(val_df)}')\n\nmodel_configs = [\n    ('vit_mae',   build_vit_model,    {'ssl_weights_path': ssl_weights_path}),\n    ('swin_tiny', build_swin_model,   {}),\n    ('convnext',  build_convnext_model, {}),\n]\n\nall_results = {}\n\nfor model_name, builder, kwargs in model_configs:\n    save_path = os.path.join(CFG.OUTPUT_DIR, f'{model_name}_best.pth')\n    model     = builder(**kwargs)\n    best_f1, history = finetune_model(model, train_df, val_df, model_name, save_path)\n    all_results[model_name] = {'best_f1': best_f1, 'save_path': save_path, 'history': history}\n    # Free GPU memory between models\n    del model\n    torch.cuda.empty_cache()\n\nprint('\\n' + '='*60)\nprint('Fine-tuning Summary:')\nfor name, res in all_results.items():\n    print(f'  {name:15s} | Best Val F1: {res[\"best_f1\"]:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T16:46:56.992607Z","iopub.execute_input":"2026-02-25T16:46:56.992871Z"}},"outputs":[{"name":"stdout","text":"Train: 720 | Val: 180\nWARNING: No SSL weights found, training ViT from scratch\n\nFine-tuning: vit_mae\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   1 | tr_loss=0.8018 tr_acc=0.2458 val_acc=0.5222 val_f1=0.1822 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   2 | tr_loss=0.7862 tr_acc=0.2792 val_acc=0.3667 val_f1=0.1896 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   3 | tr_loss=0.7859 tr_acc=0.2597 val_acc=0.4500 val_f1=0.3193 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   4 | tr_loss=0.7727 tr_acc=0.2833 val_acc=0.4556 val_f1=0.2609\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   5 | tr_loss=0.7709 tr_acc=0.2764 val_acc=0.4222 val_f1=0.3515 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   6 | tr_loss=0.7514 tr_acc=0.2944 val_acc=0.2833 val_f1=0.2120\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   7 | tr_loss=0.7493 tr_acc=0.3139 val_acc=0.4889 val_f1=0.2963\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   8 | tr_loss=0.7371 tr_acc=0.3264 val_acc=0.4556 val_f1=0.3198\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   9 | tr_loss=0.7378 tr_acc=0.3472 val_acc=0.5389 val_f1=0.3630 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  10 | tr_loss=0.7346 tr_acc=0.2861 val_acc=0.4000 val_f1=0.3114\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  11 | tr_loss=0.7117 tr_acc=0.3764 val_acc=0.5111 val_f1=0.3480\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  12 | tr_loss=0.7293 tr_acc=0.3417 val_acc=0.4444 val_f1=0.3390\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  13 | tr_loss=0.7110 tr_acc=0.2833 val_acc=0.4444 val_f1=0.3304\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  14 | tr_loss=0.7255 tr_acc=0.3194 val_acc=0.4278 val_f1=0.3391\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  15 | tr_loss=0.7266 tr_acc=0.3181 val_acc=0.4444 val_f1=0.3375\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  16 | tr_loss=0.7106 tr_acc=0.3069 val_acc=0.4611 val_f1=0.3385\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  17 | tr_loss=0.7167 tr_acc=0.3236 val_acc=0.4444 val_f1=0.3303\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  18 | tr_loss=0.6983 tr_acc=0.3306 val_acc=0.4944 val_f1=0.3599\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  19 | tr_loss=0.7057 tr_acc=0.3736 val_acc=0.4611 val_f1=0.3486\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  20 | tr_loss=0.7168 tr_acc=0.3333 val_acc=0.5056 val_f1=0.3675 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  21 | tr_loss=0.7043 tr_acc=0.3361 val_acc=0.4778 val_f1=0.3245\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  22 | tr_loss=0.7157 tr_acc=0.3486 val_acc=0.4778 val_f1=0.3200\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  23 | tr_loss=0.7107 tr_acc=0.3542 val_acc=0.5444 val_f1=0.3548\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  24 | tr_loss=0.7018 tr_acc=0.3667 val_acc=0.5611 val_f1=0.3427\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  25 | tr_loss=0.6930 tr_acc=0.3458 val_acc=0.5556 val_f1=0.3610\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  26 | tr_loss=0.6922 tr_acc=0.3792 val_acc=0.5556 val_f1=0.3603\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  27 | tr_loss=0.6890 tr_acc=0.3333 val_acc=0.5389 val_f1=0.3903 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  28 | tr_loss=0.7042 tr_acc=0.3431 val_acc=0.5000 val_f1=0.3667\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  29 | tr_loss=0.6887 tr_acc=0.3500 val_acc=0.4722 val_f1=0.3539\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  30 | tr_loss=0.6927 tr_acc=0.3333 val_acc=0.5056 val_f1=0.3710\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  31 | tr_loss=0.7198 tr_acc=0.3625 val_acc=0.5000 val_f1=0.3491\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  32 | tr_loss=0.6843 tr_acc=0.3889 val_acc=0.4944 val_f1=0.3455\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  33 | tr_loss=0.7064 tr_acc=0.3708 val_acc=0.5111 val_f1=0.3738\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  34 | tr_loss=0.6820 tr_acc=0.3264 val_acc=0.5000 val_f1=0.3667\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  35 | tr_loss=0.6840 tr_acc=0.3056 val_acc=0.4944 val_f1=0.3633\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  36 | tr_loss=0.6731 tr_acc=0.3917 val_acc=0.4944 val_f1=0.3637\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  37 | tr_loss=0.6991 tr_acc=0.3181 val_acc=0.5000 val_f1=0.3673\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  38 | tr_loss=0.6934 tr_acc=0.3764 val_acc=0.5000 val_f1=0.3673\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  39 | tr_loss=0.6950 tr_acc=0.3583 val_acc=0.5000 val_f1=0.3673\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  40 | tr_loss=0.7018 tr_acc=0.3764 val_acc=0.5000 val_f1=0.3673\n  Best val F1: 0.3903\n","output_type":"stream"},{"name":"stderr","text":"Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","output_type":"stream"},{"name":"stdout","text":"Swin-Tiny: adapted to 16-channel 64x64 input\n\nFine-tuning: swin_tiny\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   1 | tr_loss=0.9066 tr_acc=0.2444 val_acc=0.2333 val_f1=0.1871 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   2 | tr_loss=0.8481 tr_acc=0.2611 val_acc=0.3389 val_f1=0.2268 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   3 | tr_loss=0.7466 tr_acc=0.2917 val_acc=0.4500 val_f1=0.3977 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   4 | tr_loss=0.6600 tr_acc=0.4056 val_acc=0.5500 val_f1=0.4127 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   5 | tr_loss=0.6377 tr_acc=0.3736 val_acc=0.5056 val_f1=0.4221 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   6 | tr_loss=0.6000 tr_acc=0.4306 val_acc=0.5389 val_f1=0.4521 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   7 | tr_loss=0.5901 tr_acc=0.4458 val_acc=0.6333 val_f1=0.5217 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   8 | tr_loss=0.5121 tr_acc=0.4917 val_acc=0.7056 val_f1=0.4911\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   9 | tr_loss=0.5206 tr_acc=0.4944 val_acc=0.6333 val_f1=0.5132\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  10 | tr_loss=0.5173 tr_acc=0.5472 val_acc=0.5278 val_f1=0.4734\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  11 | tr_loss=0.4978 tr_acc=0.5333 val_acc=0.6111 val_f1=0.5076\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  12 | tr_loss=0.4616 tr_acc=0.5458 val_acc=0.6667 val_f1=0.5470 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  13 | tr_loss=0.4914 tr_acc=0.4569 val_acc=0.6000 val_f1=0.5184\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  14 | tr_loss=0.4833 tr_acc=0.4431 val_acc=0.5833 val_f1=0.5408\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  15 | tr_loss=0.4914 tr_acc=0.5139 val_acc=0.7667 val_f1=0.6056 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  16 | tr_loss=0.4292 tr_acc=0.5667 val_acc=0.6111 val_f1=0.5465\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  17 | tr_loss=0.4469 tr_acc=0.5889 val_acc=0.6667 val_f1=0.5407\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  18 | tr_loss=0.4204 tr_acc=0.5583 val_acc=0.7333 val_f1=0.5530\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  19 | tr_loss=0.3889 tr_acc=0.5750 val_acc=0.6611 val_f1=0.5526\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  20 | tr_loss=0.4249 tr_acc=0.5444 val_acc=0.6833 val_f1=0.5190\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  21 | tr_loss=0.4145 tr_acc=0.5653 val_acc=0.7000 val_f1=0.5788\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  22 | tr_loss=0.4301 tr_acc=0.5292 val_acc=0.6611 val_f1=0.5648\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  23 | tr_loss=0.4141 tr_acc=0.5278 val_acc=0.7389 val_f1=0.6051\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  24 | tr_loss=0.4313 tr_acc=0.5403 val_acc=0.6722 val_f1=0.5256\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  25 | tr_loss=0.4083 tr_acc=0.5889 val_acc=0.6889 val_f1=0.5584\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  26 | tr_loss=0.4172 tr_acc=0.6222 val_acc=0.6778 val_f1=0.5870\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  27 | tr_loss=0.4208 tr_acc=0.5194 val_acc=0.6722 val_f1=0.5701\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  28 | tr_loss=0.4210 tr_acc=0.6667 val_acc=0.6944 val_f1=0.5753\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  29 | tr_loss=0.3950 tr_acc=0.6528 val_acc=0.6722 val_f1=0.5538\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  30 | tr_loss=0.4252 tr_acc=0.6569 val_acc=0.7444 val_f1=0.5766\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  31 | tr_loss=0.3768 tr_acc=0.5528 val_acc=0.7333 val_f1=0.5822\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  32 | tr_loss=0.4253 tr_acc=0.5972 val_acc=0.6889 val_f1=0.5711\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  33 | tr_loss=0.3383 tr_acc=0.6194 val_acc=0.7056 val_f1=0.5583\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  34 | tr_loss=0.3700 tr_acc=0.5000 val_acc=0.7500 val_f1=0.5807\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  35 | tr_loss=0.3835 tr_acc=0.4917 val_acc=0.7056 val_f1=0.5616\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  36 | tr_loss=0.3868 tr_acc=0.6722 val_acc=0.7278 val_f1=0.5707\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  37 | tr_loss=0.3431 tr_acc=0.7361 val_acc=0.7222 val_f1=0.5665\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  38 | tr_loss=0.3861 tr_acc=0.5028 val_acc=0.7278 val_f1=0.5707\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  39 | tr_loss=0.3801 tr_acc=0.5014 val_acc=0.7278 val_f1=0.5707\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"  Epoch  40 | tr_loss=0.3748 tr_acc=0.7042 val_acc=0.7278 val_f1=0.5707\n  Best val F1: 0.6056\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55619385f7bd4be68a4ac7b4b33989ef"}},"metadata":{}},{"name":"stdout","text":"ConvNeXt-Small: adapted to 16-channel input\n\nFine-tuning: convnext\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   1 | tr_loss=0.8822 tr_acc=0.2208 val_acc=0.5500 val_f1=0.1774 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   2 | tr_loss=0.7872 tr_acc=0.2667 val_acc=0.1278 val_f1=0.1197\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   3 | tr_loss=0.7719 tr_acc=0.2500 val_acc=0.3056 val_f1=0.1378\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   4 | tr_loss=0.7525 tr_acc=0.3056 val_acc=0.2722 val_f1=0.2454 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   5 | tr_loss=0.7044 tr_acc=0.3736 val_acc=0.2111 val_f1=0.2574 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   6 | tr_loss=0.6211 tr_acc=0.4514 val_acc=0.6111 val_f1=0.5119 ✓ saved\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   7 | tr_loss=0.5742 tr_acc=0.4625 val_acc=0.5111 val_f1=0.4118\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"  Epoch   8 | tr_loss=0.5371 tr_acc=0.4361 val_acc=0.3611 val_f1=0.3567\n","output_type":"stream"},{"name":"stderr","text":"  Train epoch 9:   0%|          | 0/23 [00:00<?, ?it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## 5. Stage 3: Ensemble + Test-Time Augmentation","metadata":{}},{"cell_type":"code","source":"# ─── TTA Helper ───────────────────────────────────────────────────────────────\n\ndef tta_augment(img_tensor, aug_idx):\n    \"\"\"\n    8 TTA augmentations: 4 rotations × 2 flips.\n    img_tensor: (C, H, W)\n    \"\"\"\n    k    = aug_idx % 4          # rotation: 0, 90, 180, 270\n    flip = aug_idx // 4         # 0: no flip, 1: horizontal flip\n    \n    img = img_tensor\n    if k > 0:\n        img = torch.rot90(img, k=k, dims=[1, 2])\n    if flip:\n        img = torch.flip(img, dims=[2])\n    return img\n\n\n@torch.no_grad()\ndef predict_with_tta(model, loader, n_tta=CFG.TTA_AUGS):\n    \"\"\"\n    Get softmax predictions with TTA.\n    Returns: numpy array (N, num_classes)\n    \"\"\"\n    model.eval()\n    all_probs = []\n    \n    for imgs, _ in tqdm(loader, desc='  Predicting with TTA', leave=False):\n        B = imgs.shape[0]\n        batch_probs = torch.zeros(B, CFG.NUM_CLASSES)\n        \n        for aug_i in range(n_tta):\n            aug_imgs = torch.stack([tta_augment(imgs[j], aug_i) for j in range(B)])\n            aug_imgs = aug_imgs.to(device, non_blocking=True)\n            with torch.cuda.amp.autocast():\n                logits = model(aug_imgs)\n            batch_probs += F.softmax(logits.cpu().float(), dim=-1)\n        \n        batch_probs /= n_tta\n        all_probs.append(batch_probs.numpy())\n    \n    return np.concatenate(all_probs, axis=0)\n\n\nprint('TTA utilities defined.')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─── Ensemble Inference ───────────────────────────────────────────────────────\n\ndef load_model_for_inference(model_name, weights_path):\n    \"\"\"Load a saved model for inference.\"\"\"\n    builders = {\n        'vit_mae':   lambda: build_vit_model(ssl_weights_path),\n        'swin_tiny': build_swin_model,\n        'convnext':  build_convnext_model,\n    }\n    model = builders[model_name]()\n    state = torch.load(weights_path, map_location='cpu')\n    model.load_state_dict(state, strict=False)\n    model = model.to(device)\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n    model.eval()\n    return model\n\n\ndef run_ensemble(val_df_or_test_df, is_test=False):\n    \"\"\"\n    Run the 3-model weighted ensemble with TTA.\n    Returns final predictions and (if val) accuracy/F1.\n    \"\"\"\n    dataset = S2LabeledDataset(val_df_or_test_df, augment=False)\n    loader  = DataLoader(dataset, batch_size=CFG.FT_BATCH_SIZE * 2,\n                         shuffle=False, num_workers=4, pin_memory=True)\n    \n    model_names   = ['vit_mae', 'swin_tiny', 'convnext']\n    ens_weights   = CFG.ENSEMBLE_WEIGHTS\n    ensemble_probs = np.zeros((len(dataset), CFG.NUM_CLASSES))\n    \n    for name, w in zip(model_names, ens_weights):\n        save_path = all_results[name]['save_path']\n        if not os.path.exists(save_path):\n            print(f'  Skipping {name} (weights not found)')\n            continue\n        \n        print(f'  Loading {name} (weight={w:.2f})...')\n        model = load_model_for_inference(name, save_path)\n        probs = predict_with_tta(model, loader)\n        ensemble_probs += w * probs\n        del model\n        torch.cuda.empty_cache()\n    \n    final_preds = ensemble_probs.argmax(axis=1)\n    \n    if not is_test:\n        true_labels = val_df_or_test_df['label_idx'].values\n        acc = accuracy_score(true_labels, final_preds)\n        f1  = f1_score(true_labels, final_preds, average='macro')\n        print(f'\\nEnsemble Val Accuracy: {acc:.4f}')\n        print(f'Ensemble Val F1 (macro): {f1:.4f}')\n        print('\\nClassification Report:')\n        print(classification_report(true_labels, final_preds,\n                                     target_names=CFG.CLASS_NAMES))\n        return final_preds, acc, f1, ensemble_probs\n    \n    return final_preds, ensemble_probs\n\n\nprint('\\n' + '='*60)\nprint('STAGE 3: Ensemble + TTA Evaluation')\nprint('='*60)\nval_preds, val_acc, val_f1, val_probs = run_ensemble(val_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─── Optimize Ensemble Weights ────────────────────────────────────────────────\n# Tune weights on validation set using scipy minimize\n\nfrom scipy.optimize import minimize\n\n# Collect per-model val predictions\nmodel_val_probs = {}\nfor name in ['vit_mae', 'swin_tiny', 'convnext']:\n    save_path = all_results.get(name, {}).get('save_path', '')\n    if os.path.exists(save_path):\n        val_ds     = S2LabeledDataset(val_df, augment=False)\n        val_loader = DataLoader(val_ds, batch_size=CFG.FT_BATCH_SIZE * 2,\n                                shuffle=False, num_workers=4)\n        model = load_model_for_inference(name, save_path)\n        probs = predict_with_tta(model, val_loader)\n        model_val_probs[name] = probs\n        del model\n        torch.cuda.empty_cache()\n\ntrue_labels = val_df['label_idx'].values\nmodel_names_available = list(model_val_probs.keys())\n\ndef neg_f1(weights):\n    weights = np.array(weights)\n    weights = np.abs(weights) / np.abs(weights).sum()  # normalize to sum=1\n    combined = sum(w * model_val_probs[n] for w, n in zip(weights, model_names_available))\n    preds = combined.argmax(axis=1)\n    return -f1_score(true_labels, preds, average='macro')\n\n# Optimize\nx0      = [1/len(model_names_available)] * len(model_names_available)\nresult  = minimize(neg_f1, x0, method='Nelder-Mead',\n                   options={'maxiter': 1000, 'xatol': 1e-4})\nopt_w   = np.abs(result.x) / np.abs(result.x).sum()\n\nprint('\\nOptimized ensemble weights:')\nfor name, w in zip(model_names_available, opt_w):\n    print(f'  {name:15s}: {w:.4f}')\n\n# Apply optimized weights\ncombined = sum(w * model_val_probs[n] for w, n in zip(opt_w, model_names_available))\nopt_preds = combined.argmax(axis=1)\nopt_acc   = accuracy_score(true_labels, opt_preds)\nopt_f1    = f1_score(true_labels, opt_preds, average='macro')\nprint(f'\\nOptimized Ensemble Val Accuracy: {opt_acc:.4f}')\nprint(f'Optimized Ensemble Val F1:       {opt_f1:.4f}')\n\n# Save optimized weights for test inference\nCFG.ENSEMBLE_WEIGHTS = opt_w.tolist()\nnp.save(os.path.join(CFG.OUTPUT_DIR, 'ensemble_weights.npy'), opt_w)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Generate Test Predictions & Submission","metadata":{}},{"cell_type":"code","source":"# ─── Test Dataset ─────────────────────────────────────────────────────────────\n\nclass S2TestDataset(Dataset):\n    \"\"\"Test dataset — no labels, returns image + filename.\"\"\"\n    \n    def __init__(self, test_paths, img_size=CFG.IMG_SIZE):\n        self.paths    = test_paths\n        self.img_size = img_size\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, idx):\n        path = self.paths[idx]\n        try:\n            img = load_sentinel2_patch(path, self.img_size)\n            img = compute_vegetation_indices(img)\n        except Exception:\n            img = np.zeros((CFG.IN_CHANNELS, self.img_size, self.img_size), dtype=np.float32)\n        fname = os.path.basename(path)\n        # Return dummy label 0 so we can reuse DataLoader\n        return torch.FloatTensor(img), torch.tensor(0, dtype=torch.long)\n\n\ndef discover_test_data(root):\n    \"\"\"Find test folders — adapt this based on actual test folder structure.\"\"\"\n    test_folders = []\n    test_root = os.path.join(root, 'test')  # adjust if needed\n    if os.path.isdir(test_root):\n        for item in sorted(os.listdir(test_root)):\n            p = os.path.join(test_root, item)\n            if os.path.isdir(p):\n                test_folders.append(p)\n    return test_folders\n\n\ntest_folders = discover_test_data(CFG.LABELED_ROOT)\nprint(f'Found {len(test_folders)} test samples')\n\nif test_folders:\n    test_ds = S2TestDataset(test_folders)\n    test_loader = DataLoader(test_ds, batch_size=CFG.FT_BATCH_SIZE * 2,\n                             shuffle=False, num_workers=4)\n\n    # Ensemble over test\n    test_probs = np.zeros((len(test_folders), CFG.NUM_CLASSES))\n    for name, w in zip(model_names_available, opt_w):\n        save_path = all_results[name]['save_path']\n        model = load_model_for_inference(name, save_path)\n        probs = predict_with_tta(model, test_loader)\n        test_probs += w * probs\n        del model\n        torch.cuda.empty_cache()\n\n    test_preds = test_probs.argmax(axis=1)\n    IDX2CLASS  = {v: k for k, v in CFG.CLASS2IDX.items()}\n    pred_labels = [IDX2CLASS[p] for p in test_preds]\n    \n    # Build submission\n    submission = pd.DataFrame({\n        'Id':       [os.path.basename(f) for f in test_folders],\n        'Category': pred_labels\n    })\n    sub_path = os.path.join(CFG.OUTPUT_DIR, 'submission.csv')\n    submission.to_csv(sub_path, index=False)\n    print(f'\\nSubmission saved: {sub_path}')\n    print(submission.head(10))\n    print('\\nPrediction distribution:')\n    print(submission['Category'].value_counts())\nelse:\n    print('No test folder found — run on val data as a sanity check')\n    print(f'\\nFinal Validation F1: {opt_f1:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Hyperparameter Tuning Guide\n\n### To push from 0.90 → 0.93:\n\n**1. More SSL epochs:**\n```python\nCFG.SSL_EPOCHS = 100  # or 200 if you have time\n```\n\n**2. Pseudo-labeling (semi-supervised loop):**\nAfter first fine-tune, predict on unlabeled S2 data with high confidence threshold (>0.95), add to labeled set, retrain.\n\n**3. Run all 5 folds and ensemble fold checkpoints:**\n```python\n# Train on all 5 folds → 5 checkpoints per model → 15 models total\n# This is the strongest possible ensemble\n```\n\n**4. Add temporal averaging:**\n```python\n# For samples with multiple time acquisitions:\n# Load embeddings from T timesteps, mean-pool, then classify\n```\n\n**5. Increase image size:**\n```python\nCFG.IMG_SIZE = 128  # needs more VRAM — reduce batch size to 16\n```\n\n### Key hyperparams to tune first (highest ROI):\n| Parameter | Default | Try |\n|---|---|---|\n| `SSL_EPOCHS` | 50 | 100-200 |\n| `FT_LR` | 5e-5 | 1e-5, 1e-4 |\n| `SSL_MASK_RATIO` | 0.75 | 0.6, 0.8 |\n| `FT_BATCH_SIZE` | 32 | 16, 64 |\n| `IMG_SIZE` | 64 | 128 |","metadata":{}}]}